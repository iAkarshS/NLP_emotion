{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7986fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8e611ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: go_emotions/simplified\n",
      "Found cached dataset go_emotions (C:/Users/AKARSH S/.cache/huggingface/datasets/go_emotions/simplified/0.0.0/2637cfdd4e64d30249c3ed2150fa2b9d279766bfcd6a809b9f085c61a90d776d)\n",
      "No config specified, defaulting to: go_emotions/simplified\n",
      "Found cached dataset go_emotions (C:/Users/AKARSH S/.cache/huggingface/datasets/go_emotions/simplified/0.0.0/2637cfdd4e64d30249c3ed2150fa2b9d279766bfcd6a809b9f085c61a90d776d)\n",
      "No config specified, defaulting to: go_emotions/simplified\n",
      "Found cached dataset go_emotions (C:/Users/AKARSH S/.cache/huggingface/datasets/go_emotions/simplified/0.0.0/2637cfdd4e64d30249c3ed2150fa2b9d279766bfcd6a809b9f085c61a90d776d)\n"
     ]
    }
   ],
   "source": [
    "#load dtaset\n",
    "from datasets import load_dataset\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "DFtrain = pd.DataFrame(load_dataset('go_emotions', split = 'train'))\n",
    "DFtest = pd.DataFrame(load_dataset('go_emotions', split = 'test'))\n",
    "DFval = pd.DataFrame(load_dataset('go_emotions', split = 'validation'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d56509b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My favourite food is anything I didn't have to cook myself.</td>\n",
       "      <td>[27]</td>\n",
       "      <td>eebbqej</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Now if he does off himself, everyone will think hes having a laugh screwing with people instead of actually dead</td>\n",
       "      <td>[27]</td>\n",
       "      <td>ed00q6i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WHY THE FUCK IS BAYLESS ISOING</td>\n",
       "      <td>[2]</td>\n",
       "      <td>eezlygj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>To make her feel threatened</td>\n",
       "      <td>[14]</td>\n",
       "      <td>ed7ypvh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dirty Southern Wankers</td>\n",
       "      <td>[3]</td>\n",
       "      <td>ed0bdzj</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                               text  \\\n",
       "0                                                       My favourite food is anything I didn't have to cook myself.   \n",
       "1  Now if he does off himself, everyone will think hes having a laugh screwing with people instead of actually dead   \n",
       "2                                                                                    WHY THE FUCK IS BAYLESS ISOING   \n",
       "3                                                                                       To make her feel threatened   \n",
       "4                                                                                            Dirty Southern Wankers   \n",
       "\n",
       "  labels       id  \n",
       "0   [27]  eebbqej  \n",
       "1   [27]  ed00q6i  \n",
       "2    [2]  eezlygj  \n",
       "3   [14]  ed7ypvh  \n",
       "4    [3]  ed0bdzj  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DFtrain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18103415",
   "metadata": {},
   "outputs": [],
   "source": [
    "DFtrain = DFtrain.append(DFval).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40ae291b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DFtrain = DFtrain.append(DFtest).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4521134",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_TRAIN = DFtrain.drop(['id'],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80b8e7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_dict = {0: \"admiration\", \n",
    "                 1: \"amusement\", \n",
    "                 2: \"anger\", \n",
    "                 3: \"annoyance\", \n",
    "                 4: \"approval\",\n",
    "                 5: \"caring\",\n",
    "                 6: \"confusion\",\n",
    "                 7: \"curiosity\", \n",
    "                 8: \"desire\",\n",
    "                 9: \"disappointment\",\n",
    "                 10: \"disapproval\",\n",
    "                 11: \"disgust\",\n",
    "                 12: \"embarrassment\",\n",
    "                 13: \"excitement\",\n",
    "                 14: \"fear\",\n",
    "                 15: \"gratitude\",\n",
    "                 16: \"grief\",\n",
    "                 17: \"joy\",\n",
    "                 18: \"love\",\n",
    "                 19: \"nervousness\",\n",
    "                 20: \"optimism\",\n",
    "                 21: \"pride\",\n",
    "                 22: \"realization\",\n",
    "                 23: \"relief\",\n",
    "                 24: \"remorse\",\n",
    "                 25: \"sadness\",\n",
    "                 26: \"surprise\",\n",
    "                 27: \"neutral\"\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c117ec92",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_TRAIN['labels_str'] = DF_TRAIN['labels'].apply(lambda tpl: [emotions_dict.get(x) for x in tpl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53e63e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_for_values = {\n",
    "    'admiration': 'admiration', \n",
    "    'gratitude': 'admiration', \n",
    "    'anger': 'anger',\n",
    "    'annoyance':'anger',\n",
    "    'approval': 'approval',\n",
    "    'relief': 'approval',\n",
    "    'curiosity': 'curiosity',\n",
    "    'confusion': 'curiosity',\n",
    "    'desire': 'curiosity',\n",
    "    'disgust': 'disgust',\n",
    "    'embarrassment': 'disgust',\n",
    "    'joy': 'joy',\n",
    "    'amusement': 'joy',\n",
    "    'excitement': 'joy',\n",
    "    'love': 'love',\n",
    "    'caring': 'love',\n",
    "    'nervousness': 'fear',\n",
    "    'fear': 'fear',\n",
    "    'optimism': 'optimism',\n",
    "    'pride': 'pride',\n",
    "    'disappointment': 'disappointment', \n",
    "    'disapproval': 'disappointment', \n",
    "    'surprise': 'surprise',\n",
    "    'realization': 'surprise',\n",
    "    'neutral': 'neutral',\n",
    "    'grief': 'grief',\n",
    "    'sadness': 'grief',\n",
    "    'remorse': 'grief'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b8dbede",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_TRAIN['labels_merged'] = DF_TRAIN['labels_str'].apply(lambda tpl: [target_for_values.get(x) for x in tpl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3056e4d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "      <th>labels_str</th>\n",
       "      <th>labels_merged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My favourite food is anything I didn't have to cook myself.</td>\n",
       "      <td>[27]</td>\n",
       "      <td>[neutral]</td>\n",
       "      <td>[neutral]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Now if he does off himself, everyone will think hes having a laugh screwing with people instead of actually dead</td>\n",
       "      <td>[27]</td>\n",
       "      <td>[neutral]</td>\n",
       "      <td>[neutral]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WHY THE FUCK IS BAYLESS ISOING</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[anger]</td>\n",
       "      <td>[anger]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>To make her feel threatened</td>\n",
       "      <td>[14]</td>\n",
       "      <td>[fear]</td>\n",
       "      <td>[fear]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dirty Southern Wankers</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[annoyance]</td>\n",
       "      <td>[anger]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                               text  \\\n",
       "0                                                       My favourite food is anything I didn't have to cook myself.   \n",
       "1  Now if he does off himself, everyone will think hes having a laugh screwing with people instead of actually dead   \n",
       "2                                                                                    WHY THE FUCK IS BAYLESS ISOING   \n",
       "3                                                                                       To make her feel threatened   \n",
       "4                                                                                            Dirty Southern Wankers   \n",
       "\n",
       "  labels   labels_str labels_merged  \n",
       "0   [27]    [neutral]     [neutral]  \n",
       "1   [27]    [neutral]     [neutral]  \n",
       "2    [2]      [anger]       [anger]  \n",
       "3   [14]       [fear]        [fear]  \n",
       "4    [3]  [annoyance]       [anger]  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF_TRAIN.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43e01dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_TRAIN['Len_of_classes'] = DF_TRAIN['labels'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "793e6cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_TRAIN['labels_str'] = DF_TRAIN['labels_str'].astype(str).str.replace(r'\\[|\\]|,', '')\n",
    "DF_TRAIN['labels_merged'] = DF_TRAIN['labels_merged'].astype(str).str.replace(r'\\[|\\]|,', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65ee1fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_LABEL = DF_TRAIN.assign(labels_merged=DF_TRAIN['labels_merged'].str.split()).explode('labels_merged')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7548f6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_LABEL = DF_LABEL.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a147f759",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_LABEL = DF_LABEL.astype(str).drop_duplicates(subset=['text', 'labels', 'labels_str'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79eab290",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_LABELCLASS = DF_LABEL.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b7e4af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_LABELCLASS = DF_LABELCLASS.drop('Len_of_classes', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e2a0882",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "wr = ' '.join(DF_LABELCLASS['labels_merged']).lower().split()\n",
    "sw = set(STOPWORDS)\n",
    "words = [wr for wr in wr if wr not in sw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0eefcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "wf = {}\n",
    "for wrd in wr:\n",
    "    if wrd not in wf:\n",
    "        wf[wrd] = 1\n",
    "    else:\n",
    "        wf[wrd] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f60903c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "from collections import Counter\n",
    "\n",
    "def top_fty(doc, k=50):\n",
    "    dc_wrd = DF_LABELCLASS['text']\n",
    "    fty = Counter([word.lower() for word in dc_wrd])\n",
    "    wrd = fty.most_common(k)\n",
    "\n",
    "    return wrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d67d4c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\AKARSH\n",
      "[nltk_data]     S\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\AKARSH\n",
      "[nltk_data]     S\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\AKARSH\n",
      "[nltk_data]     S\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\AKARSH\n",
      "[nltk_data]     S\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS, TfidfVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "import nltk,re\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "stp_wrd = set(stopwords.words('english'))\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "ps = nltk.PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a6e14db",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_LABELCLASS = DF_LABELCLASS.drop(['labels','labels_str'],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "769c77f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "stemmer = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2d49305d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_stemming(texts): \n",
    "    Tlist=[]\n",
    "    for sentence in texts:\n",
    "        if isinstance(sentence, str) == False:\n",
    "            sentence = \"\"\n",
    "            \n",
    "        fltSntnc=[]\n",
    "        # change to lowercase\n",
    "        sentence = sentence.lower() \n",
    "        # whitespace removel\n",
    "        sentence = sentence.strip() \n",
    "        # extra spaces and tabs are removed\n",
    "        sentence = re.sub('\\s+', ' ', sentence) \n",
    "        # markup of html removed\n",
    "        sentence = re.compile('<.*?>').sub('', sentence) \n",
    "        # word [NAME] is frequently appearing so that will be removed\n",
    "        sentence = re.sub(r'\\[name\\]', '', sentence) \n",
    "        # emoticons, symbols & pictographs, transport & map symbols, flags\n",
    "        emoji_pattern = re.compile(\"[\" u\"\\U0001F600-\\U0001F64F\" \n",
    "                                   u\"\\U0001F300-\\U0001F5FF\" \n",
    "                                   u\"\\U0001F680-\\U0001F6FF\" \n",
    "                                   u\"\\U0001F1E0-\\U0001F1FF\" \n",
    "                                                       \"]+\", flags=re.UNICODE)\n",
    "        sentence = emoji_pattern.sub(r'', sentence)\n",
    "\n",
    "\n",
    "    \n",
    "        tokenizer = WordPunctTokenizer()\n",
    "        \n",
    "        for word in tokenizer.tokenize(sentence):\n",
    "            if(not word.isnumeric()) and (len(word)>2) and (word not in stop_words):  \n",
    "                  fltSntnc.append(stemmer.stem(word))   # Stem and add to filtered list\n",
    "        final_string = \" \".join(fltSntnc) # final string of cleaned words\n",
    " \n",
    "        Tlist.append(final_string)\n",
    "    \n",
    "        \n",
    "    return Tlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1f091067",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(DF_LABEL, test_size = 0.10, stratify = DF_LABEL['labels_merged'], random_state=324)# Applying the preprocessing on the test and train split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "52eafc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['text'] = preprocessing_stemming(X_train[\"text\"])\n",
    "X_test['text'] = preprocessing_stemming(X_test[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1524031b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(lowercase= True, max_features=1000, stop_words=list(ENGLISH_STOP_WORDS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2179f6eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(max_features=1000,\n",
       "                stop_words=['some', 'in', 'afterwards', 'as', 'something', 'us',\n",
       "                            'give', 'hereafter', 'namely', 'yours', 'couldnt',\n",
       "                            'and', 'go', 'these', 'those', 'whenever',\n",
       "                            'against', 'third', 'its', 'now', 'twelve',\n",
       "                            'within', 'ie', 'besides', 'somehow', 'ever',\n",
       "                            'interest', 'mostly', 'you', 'whence', ...])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.fit(X_train.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "018e4500",
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDFtrain = tfidf.transform(X_train.text)\n",
    "TFIDFtest  = tfidf.transform(X_test.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ea01ab46",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_LR = LogisticRegression(solver='sag',penalty='l2', C=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c60fae5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10, solver='sag')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_LR.fit(TFIDFtrain, X_train.labels_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "54ab0154",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainPrediction = model_LR.predict(TFIDFtrain)\n",
    "testPrediction = model_LR.predict(TFIDFtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2d392d13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5628762198253724"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# f1 score on train data\n",
    "LRSCR = f1_score(y_true= X_train.labels_merged, y_pred= trainPrediction, average='micro')\n",
    "LRSCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1134f74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps= [('tfidf', TfidfVectorizer(lowercase=True,\n",
    "                                                      max_features=1000,\n",
    "                                                      stop_words= ENGLISH_STOP_WORDS)),\n",
    "                            ('model', LogisticRegression(solver='sag',penalty='l2', C=10))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6419aa75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf',\n",
       "                 TfidfVectorizer(max_features=1000,\n",
       "                                 stop_words=frozenset({'a', 'about', 'above',\n",
       "                                                       'across', 'after',\n",
       "                                                       'afterwards', 'again',\n",
       "                                                       'against', 'all',\n",
       "                                                       'almost', 'alone',\n",
       "                                                       'along', 'already',\n",
       "                                                       'also', 'although',\n",
       "                                                       'always', 'am', 'among',\n",
       "                                                       'amongst', 'amoungst',\n",
       "                                                       'amount', 'an', 'and',\n",
       "                                                       'another', 'any',\n",
       "                                                       'anyhow', 'anyone',\n",
       "                                                       'anything', 'anyway',\n",
       "                                                       'anywhere', ...}))),\n",
       "                ('model', LogisticRegression(C=10, solver='sag'))])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(X_train.text, X_train.labels_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8401eebf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"'admiration'\", \"'anger'\", \"'approval'\", \"'curiosity'\",\n",
       "       \"'disappointment'\", \"'disgust'\", \"'fear'\", \"'grief'\", \"'joy'\",\n",
       "       \"'love'\", \"'neutral'\", \"'optimism'\", \"'pride'\", \"'surprise'\"],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3298a4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing the Intent for the given value of input\n",
    "text = [\"i want to book a hotel in florida\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "be56c249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"'neutral'\"], dtype=object)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.predict(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "869751e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = [\"WHY THE FUCK IS BAYLESS ISOING\t\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f4d301cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"'anger'\"], dtype=object)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.predict(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ef3619e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "new_file=open('testmodel.pckl','wb')\n",
    "pickle.dump(pipeline,new_file)\n",
    "new_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc0b2fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
